{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steven Rae - Machine Learning Capstone Project - Step 2\n",
    "\n",
    "## Using Naive Bayes Classification Technique\n",
    "\n",
    "**Question to be Answered:** Focusing on Essay no. 7 (On a typical Friday night I am....), want to create\n",
    "a model that predicts if a submitted essay describes outgoing activites or\n",
    "introverted/stay at home activites occurring on a Friday night.\n",
    "\n",
    "Define inputs and load the OKCupid data into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "okcupidDF = pd.read_csv(\"profiles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My review of the essay quetion content in Step 1 showed that all essay questions are provided in completely lower case. Therefore no need to write code specifically to change text to all lower case. However, Essay 7 text does need to be checked for any Nan entries and replace them with a space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "okcupidDF['essay7'] = okcupidDF['essay7'].replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine which of the essays could be labeled as 'Outgoing' and which could be classified as 'Stay at home'... I decided to create a list of search phrases that would typically be found in an essay describing 'Outgoing' activities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outgoing_search_phrases = ['friend', 'friends', 'out with', 'out and about',\n",
    "'out in the', 'out at', 'family', 'children', 'club', 'clubbing', 'hanging out with',\n",
    "'hanging with', 'concert', 'show', 'good company', 'dancing', 'bar',\n",
    "'bar hopping', 'night life', 'nightlife', 'kids', 'out to dinner', 'out for dinner', 'out on the town', 'event',\n",
    "'live music', 'socializing', 'go out', 'going out', 'happy hour', 'party']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to add a label column to the dataframe that has 1 for each essay 7 considered 'Outgoing' and 0 for 'Stay at home'. This will be considered the 'correct' labels to be compared to the labels predicted by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "Name: is_outgoing, dtype: int64\n",
      "1    31141\n",
      "0    28805\n",
      "Name: is_outgoing, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pattern = '|'.join(outgoing_search_phrases)\n",
    "\n",
    "# add label column to dataframe\n",
    "okcupidDF['is_outgoing'] = np.where(okcupidDF['essay7'].str.contains(pattern), 1, 0)\n",
    "\n",
    "print(okcupidDF['is_outgoing'].head())\n",
    "\n",
    "# do a quick check for values and NaNs just in case\n",
    "print(okcupidDF.is_outgoing.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming my choices of search phrases are resonably good, looks like I've got a pretty balanced dataset between 'outgoing' and 'stay at home' entries. I saved each type ( 1 and 0 ) to separate csv files. I spent about 15 minutes per file eyeballing the essays and I would say that the classification is pretty good if I do say so myself...:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe that's a subset of the okcupidDF. This new dataframe will be the input to the classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = okcupidDF[['essay7', 'is_outgoing']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and validation sets, validation set should be 25% of data by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, validation_set = train_test_split(feature_data, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the Naive Bayes Classifier Model and run the predictions and probabilities for the validation set and training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = CountVectorizer()\n",
    "\n",
    "counter.fit(training_set['essay7'])\n",
    "# print(counter.vocabulary_)\n",
    "training_counts = counter.transform(training_set['essay7'])\n",
    "validation_counts = counter.transform(validation_set['essay7'])\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(training_counts, training_set['is_outgoing'])\n",
    "predictions_validation = classifier.predict(validation_counts)\n",
    "probabilities_validation = classifier.predict_proba(validation_counts)\n",
    "predictions_training = classifier.predict(training_counts)\n",
    "probabilities_training = classifier.predict_proba(training_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at Accuracy for the validation predictions. I'm making an assumption that if the probability associated with the prediction is above 75%, then the prediction is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Validation Set -------\n",
      "Total Predictions:  14987\n",
      "Total Outgoing Predictions:  11990\n",
      "Total Outgoing Predictions Correct:  8156\n",
      "Total Introverted Predictions:  2997\n",
      "Total Introverted Predictions Correct:  2127\n",
      "Total Predictions Correct:  10283\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "total_predictions = 0\n",
    "total_predictions_correct = 0\n",
    "outgoing_predictions = 0\n",
    "outgoing_predictions_correct = 0\n",
    "introverted_predictions = 0\n",
    "introverted_predictions_correct = 0\n",
    "\n",
    "while i < len(predictions_validation):\n",
    "    if predictions_validation[i] == 1:\n",
    "        outgoing_predictions += 1\n",
    "        total_predictions += 1\n",
    "        if probabilities_validation[i][1] > 0.75:\n",
    "            outgoing_predictions_correct += 1\n",
    "            total_predictions_correct += 1\n",
    "    if predictions_validation[i] == 0:\n",
    "        introverted_predictions += 1\n",
    "        total_predictions += 1\n",
    "        if probabilities_validation[i][0] > 0.75:\n",
    "            introverted_predictions_correct += 1\n",
    "            total_predictions_correct += 1\n",
    "    i += 1\n",
    "\n",
    "print('------- Validation Set -------')\n",
    "print('Total Predictions: ', total_predictions)\n",
    "print('Total Outgoing Predictions: ', outgoing_predictions)\n",
    "print('Total Outgoing Predictions Correct: ', outgoing_predictions_correct)\n",
    "print('Total Introverted Predictions: ', introverted_predictions)\n",
    "print('Total Introverted Predictions Correct: ', introverted_predictions_correct)\n",
    "print('Total Predictions Correct: ', total_predictions_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at Precision, Recall an F1 for the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.38      0.54      7220\n",
      "           1       0.63      0.97      0.76      7767\n",
      "\n",
      "   micro avg       0.69      0.69      0.69     14987\n",
      "   macro avg       0.77      0.68      0.65     14987\n",
      "weighted avg       0.77      0.69      0.66     14987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_set['is_outgoing'], predictions_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at Accuracy for the training predictions. I'm making an assumption that if the probability associated with the prediction is above 75%, then the prediction is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Training Set -------\n",
      "Total Predictions:  44959\n",
      "Total Outgoing Predictions:  35430\n",
      "Total Outgoing Predictions Correct:  24346\n",
      "Total Introverted Predictions:  9529\n",
      "Total Introverted Predictions Correct:  7319\n",
      "Total Predictions Correct:  31665\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "total_predictions = 0\n",
    "total_predictions_correct = 0\n",
    "outgoing_predictions = 0\n",
    "outgoing_predictions_correct = 0\n",
    "introverted_predictions = 0\n",
    "introverted_predictions_correct = 0\n",
    "\n",
    "while i < len(predictions_training):\n",
    "    if predictions_training[i] == 1:\n",
    "        outgoing_predictions += 1\n",
    "        total_predictions += 1\n",
    "        if probabilities_training[i][1] > 0.75:\n",
    "            outgoing_predictions_correct += 1\n",
    "            total_predictions_correct += 1\n",
    "    if predictions_training[i] == 0:\n",
    "        introverted_predictions += 1\n",
    "        total_predictions += 1\n",
    "        if probabilities_training[i][0] > 0.75:\n",
    "            introverted_predictions_correct += 1\n",
    "            total_predictions_correct += 1\n",
    "    i += 1\n",
    "\n",
    "print('------- Training Set -------')\n",
    "print('Total Predictions: ', total_predictions)\n",
    "print('Total Outgoing Predictions: ', outgoing_predictions)\n",
    "print('Total Outgoing Predictions Correct: ', outgoing_predictions_correct)\n",
    "print('Total Introverted Predictions: ', introverted_predictions)\n",
    "print('Total Introverted Predictions Correct: ', introverted_predictions_correct)\n",
    "print('Total Predictions Correct: ', total_predictions_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at Precision, Recall an F1 for the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.41      0.57     21585\n",
      "           1       0.64      0.97      0.77     23374\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     44959\n",
      "   macro avg       0.79      0.69      0.67     44959\n",
      "weighted avg       0.78      0.70      0.68     44959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(training_set['is_outgoing'], predictions_training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Conclusions:\n",
    "\n",
    "Looks like accuracy for both the training and validation sets was around 70%. Since the 'outgoing' vs 'stay at home' dataset is reasonably balanced. Precision suggests that our model is really good at predicting 'stay at home' but not so good at predicting the 'outgoing' activities. This is most likely due to the way I set this up. There is certainly more specivity is looking for 'outgoing' activities.\n",
    "\n",
    "I'm starting to think that this specivity in looking for 'outgoing' activities is certainly showing up in the recall info as well. Perhaps I've inadvertantly introduced bias into the dataset. The more balanced F1 score seems to be reflecting this as well.\n",
    "\n",
    "Some of this bias may have been prevented if OK Cupid could have asked their clients if they considered themselves 'outgoing' or 'introverted' and I could have used that as the label to be predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
